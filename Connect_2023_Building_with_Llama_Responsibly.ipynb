{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIynFmqnbq42gnqaMXl8BD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15eed1f9d6d74914a5d8e6eb01b4037e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31ee0c61bc5748c7b2b9db40534a4575",
              "IPY_MODEL_bc3df46282f3490e9fb0b409df095c9e",
              "IPY_MODEL_4fd04e25646e4f91b217926e31e1b6bf"
            ],
            "layout": "IPY_MODEL_2158516fe1f143bc82031af74368fcae"
          }
        },
        "31ee0c61bc5748c7b2b9db40534a4575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fefa7084474b4443aac9c1e6144dda2d",
            "placeholder": "​",
            "style": "IPY_MODEL_8c8dc6db734347fc872f560ee35011ca",
            "value": "Downloading (…)chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "bc3df46282f3490e9fb0b409df095c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c733f756b0174c54a9eb19da988b2df1",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cefbbab56400477a917c87028475797a",
            "value": 9763701888
          }
        },
        "4fd04e25646e4f91b217926e31e1b6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc369d0e2b74ea7b8ffcb856d9189b7",
            "placeholder": "​",
            "style": "IPY_MODEL_cdc60fbba8734004baad30a3400416c4",
            "value": " 9.76G/9.76G [01:22&lt;00:00, 76.9MB/s]"
          }
        },
        "2158516fe1f143bc82031af74368fcae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fefa7084474b4443aac9c1e6144dda2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c8dc6db734347fc872f560ee35011ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c733f756b0174c54a9eb19da988b2df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cefbbab56400477a917c87028475797a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dc369d0e2b74ea7b8ffcb856d9189b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc60fbba8734004baad30a3400416c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitsangani/llama/blob/main/Connect_2023_Building_with_Llama_Responsibly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: This outline needs to be grounded in responsibility more. Reference the [RUG](https://ai.meta.com/static-resource/responsible-use-guide/). Call out pieces of the guide and how to use it in building\n"
      ],
      "metadata": {
        "id": "Vd4KE8ckXwIM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## **Hello Llama 2**\n",
        "</div>\n",
        "\n",
        "Welcome to the next generation of our open source large language model! Llama 2 release includes model weights and starting code for pretrained and fine-tuned Llama language models — ranging from 7B to 70B parameters.\n",
        "\n",
        "We will start with basics of Llama 2, key features, how to get the models from Meta’s site, and Cloud Service providers (Azure, AWS and GCP)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Llama 2?**\n",
        "\n",
        "Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. Llama 2 is intended for commercial and research use in English. It comes in a range of parameter sizes—7 billion, 13 billion, and 70 billion—as well as pre-trained and fine-tuned variations. Llama 2 was pre-trained on 2 trillion tokens of data from publicly available sources.\n",
        "\n",
        "---\n",
        "\n",
        "### **Get Started:**\n",
        "\n",
        "1.   Download models from Meta's [Llama site](https://ai.meta.com/llama/), [Hugging Face](https://huggingface.co/meta-llama) or directly provision an instance using it from our cloud partners [Azure](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/introducing-llama-2-on-azure/ba-p/3881233), [AWS Sagemaker](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/) or Google Cloud.\n",
        "2.   Please review the [Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/) to ensure you understand  best practices and considerations for building products powered by Llama 2 in a responsible manner.\n",
        "3. Please also review [Acceptable Use Policy](https://ai.meta.com/llama/use-policy/) to ensure safe and fair use. If you access or use Llama 2, you agree to this Acceptable Use Policy.\n",
        "4. Please download the [code](https://github.com/facebookresearch/llama/) from Meta's Github site.  \n",
        "\n",
        "> * Quickly go over technical review of Llama from another opened tab - https://ai.meta.com/llama/ *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PGPSI3M5PGTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Get a quantized Llama 13B model from Hugging Face**\n",
        "\n",
        "To run on Google Colab, free tier - we will need to run a quantized model. There are quantized models available on Hugging Face which allow us to utilize the model on a T4 GPU. We will use the [Llama-2-13B-GGML model](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML). We will use the model based on the GGLM library and leverage [Llama CPP](https://github.com/ggerganov/llama.cpp)."
      ],
      "metadata": {
        "id": "iXCZYnbfRfOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install all the required packages**"
      ],
      "metadata": {
        "id": "K4maoc_cScwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "-dYStdFqSmGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the models\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "XAOKmM_ASs-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the model path\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "A3ZOaGqoTlVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import the libraries and download the model**"
      ],
      "metadata": {
        "id": "N49nbo2dT5Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "15eed1f9d6d74914a5d8e6eb01b4037e",
            "31ee0c61bc5748c7b2b9db40534a4575",
            "bc3df46282f3490e9fb0b409df095c9e",
            "4fd04e25646e4f91b217926e31e1b6bf",
            "2158516fe1f143bc82031af74368fcae",
            "fefa7084474b4443aac9c1e6144dda2d",
            "8c8dc6db734347fc872f560ee35011ca",
            "c733f756b0174c54a9eb19da988b2df1",
            "cefbbab56400477a917c87028475797a",
            "9dc369d0e2b74ea7b8ffcb856d9189b7",
            "cdc60fbba8734004baad30a3400416c4"
          ]
        },
        "id": "OseJDPE-Tw_T",
        "outputId": "8957417b-cda0-49cc-cd53-467fae936e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15eed1f9d6d74914a5d8e6eb01b4037e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load the model**"
      ],
      "metadata": {
        "id": "dkFQlseWUsRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWP6inyJVDLR",
        "outputId": "adc609bd-58bb-4ba6-ffe3-7b8211d61a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the number of layers in GPU\n",
        "\n",
        "lcpp_llm.params.n_gpu_layers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYCr8tbnVLOn",
        "outputId": "5cf5bf5c-3ad1-4507-8c5f-7d53d26b4cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create a prompt template**"
      ],
      "metadata": {
        "id": "SwcKYtpBVxUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me more about life and how to be successful\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "8XjmgNeVV5KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple utility to wrap text in colab before generating a response\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "kF-vDEKAZWxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate a response**"
      ],
      "metadata": {
        "id": "HVn_aHRdWF8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=lcpp_llm(\n",
        "    prompt=prompt_template,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=150,\n",
        "    echo=True)\n",
        "\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "zpJnxIvpWIw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESo5R05VWpts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qplBHgOBrPdw"
      },
      "source": [
        "## **Inference APIs**\n",
        "</div>\n",
        "\n",
        "In this section, we’ll go through different approaches to running inference of the Llama2 models. Once you have received access to the models after signing up through Meta's official form, you will be ready to run inferencing on the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxUZdUJHUsUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwYYXK4ssNu"
      },
      "source": [
        "## **Basic Chat Completion**\n",
        "</div>\n",
        "\n",
        "We will discuss differences between pre-trained and fine-tuned models and walk through the “chat completion” example to get expected features and performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G-VxBZBUs4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn3L05GYswes"
      },
      "source": [
        "## **Enhance Chat Completion and Memory**\n",
        "</div>\n",
        "\n",
        "We will then enhance the chat completion example and discuss conversation context and short-term memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NSBRj0JtN6s"
      },
      "source": [
        "## **AI Chatbot and Langchain**\n",
        "</div>\n",
        "\n",
        "Convert your example above into an AI-Chatbot using Langchain, a popular framework for building applications using Large Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRQzj-BZtOJI"
      },
      "source": [
        "## **Vector DB, Similarity Search (FAISS) and Embeddings**\n",
        "</div>\n",
        "\n",
        "In this section, we’ll go through different prompt engineering principles using Vector databases and embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUdeJ931tORu"
      },
      "source": [
        "## **Q&A Retriever with LangChain**\n",
        "</div>\n",
        "\n",
        "Complete the AI-Chatbot with Q&A retriever and walk-through the entire codebase again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PNoB3AAtOWe"
      },
      "source": [
        "## **Advanced chatbot concepts**\n",
        "</div>\n",
        "\n",
        "Explain advanced chatbot concepts (without code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Responsible Use**\n",
        "\n",
        "Safety & Moderation. Refer to RUG"
      ],
      "metadata": {
        "id": "qnEtzQa1XLls"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDQmoo6ktOZ-"
      },
      "source": [
        "## **Llama deployments**\n",
        "</div>\n",
        "\n",
        "And discuss how to deploy your AI-Chatbot for world to use (including showing this was done with llama-2-7b-chat on-device)\n"
      ]
    }
  ]
}